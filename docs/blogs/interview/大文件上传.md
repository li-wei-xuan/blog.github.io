---
title: 大文件上传
date: 2024-03-28
tags:
 - interview
categories:
 - interview
---
## 大文件上传
一次性上传会存在的问题：上传速度慢，容易超时，上传失败导致用户体验差。
:::tip
断点续传  
  允许在上传中断后继续上传，而无需从头开始。 

分片上传  
  分片上传是将大文件分割成多个分片，然后逐个上传分片。或者可以并行上传，从而提高上传速度。 

秒传  
  秒传就是用户在上传文件时，当服务器存在完全相同的文件，跳过上传过程。  
:::

## 上传流程
```
1、检验文件是否上传(秒传)
2、文件进行切片
3、对分片进行md5加密
4、上传分片(断点续传)
    1）边切片边上传
    2）切完后并发上传
5、合并切片，上报接口
```

## 核心代码
### index.html
```javascript
<input type="file" name="input" id="file" />

import { cutFile } from "./cutfile.js";

const inpFile = document.querySelector('input[type="file"]');

inpFile.onchange = async (e) => {
  const file = e.target.files[0];
  console.time("cutFile");
  const chunks = await cutFile(file);
  console.timeEnd("cutFile");
  console.log(chunks);
};
```

### cutfile.js
```javascript
const CHUNK_SIZE = 1024 * 1024 * 5; // 5MB
const THREAD_COUNT = navigator.hardwareConcurrency || 4;

export function cutFile(file) {
  return new Promise((resolve) => {
    const chunkCount = Math.ceil(file.size / CHUNK_SIZE);
    const threadChunkCount = Math.ceil(chunkCount / THREAD_COUNT);
    const result = [];
    let finishCount = 0;
    for (let i = 0; i < THREAD_COUNT; i++) {
      console.log({ i });
      // 创建一个线程，并分配任务
      // 注意：引入绝对路径，否则woker无效
      const worker = new Worker("../js/worker.js", {
        type: "module",
      });

      let end = (i + 1) * threadChunkCount;
      const start = i * threadChunkCount;
      if (end > chunkCount) {
        end = chunkCount;
      }
      worker.postMessage({
        file,
        CHUNKSIZE: CHUNK_SIZE,
        startChunkIndex: start,
        endChunkIndex: end,
      });
      worker.onmessage = (e) => {
        console.log({ e });
        for (let i = start; i < end; i++) {
          result[i] = e.data[i - start];
        }
        worker.terminate();
        finishCount++;
        if (finishCount === THREAD_COUNT) {
          resolve(result);
        }
      };
    }
  });
}
```

### worker.js
```javascript
import { createChunk } from "./createChunk.js";

onmessage = async (e) => {
  console.log(e);
  const {
    file,
    CHUNK_SIZE,
    startChunkIndex: start,
    endChunkIndex: end,
  } = e.data;
  const proms = [];
  for (let i = start; i < end; i++) {
    proms.push(createChunk(file, i, CHUNK_SIZE));
  }
  const chunks = await Promise.all(proms);
  postMessage(chunks);
};
```

### createChunk.js
```javascript
import SparkMD5 from './sparkmd5.js';
export function createChunk(file, index, chunkSize) {
  return new Promise((resolve) => {
    const start = index * chunkSize;
    const end = start + chunkSize;
    const spark = new SparkMD5.ArrayBuffer();
    const fileReader = new FileReader();
    const blob = file.slice(start, end);
    fileReader.onload = (e) => {
      spark.append(e.target.result);
      resolve({
        start,
        end,
        index,
        hash: spark.end(),
        blob,
      });
    };
    fileReader.readAsArrayBuffer(blob);
  });
}
```

